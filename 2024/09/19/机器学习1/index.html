<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习1 | zx's Blog</title><meta name="author" content="zx"><meta name="copyright" content="zx"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习入门1.常见算法有监督学习算法 分类（Classification） 回归（Regression）  knn根据当前样本跟其他样本的距离来对当前样本进行分类，超参数k用于圈定范围     123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习1">
<meta property="og:url" content="https://zx2023qj.github.io/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/index.html">
<meta property="og:site_name" content="zx&#39;s Blog">
<meta property="og:description" content="机器学习入门1.常见算法有监督学习算法 分类（Classification） 回归（Regression）  knn根据当前样本跟其他样本的距离来对当前样本进行分类，超参数k用于圈定范围     123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zx2023qj.github.io/img/3.jpg">
<meta property="article:published_time" content="2024-09-19T13:38:09.000Z">
<meta property="article:modified_time" content="2024-09-19T13:53:50.100Z">
<meta property="article:author" content="zx">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="ai">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zx2023qj.github.io/img/3.jpg"><link rel="shortcut icon" href="/img/3.jpg"><link rel="canonical" href="https://zx2023qj.github.io/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习1',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-19 21:53:50'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/3.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/2.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="zx's Blog"><span class="site-name">zx's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-19T13:38:09.000Z" title="发表于 2024-09-19 21:38:09">2024-09-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-19T13:53:50.100Z" title="更新于 2024-09-19 21:53:50">2024-09-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/">ai</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习1"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="机器学习入门"><a href="#机器学习入门" class="headerlink" title="机器学习入门"></a>机器学习入门</h1><h2 id="1-常见算法"><a href="#1-常见算法" class="headerlink" title="1.常见算法"></a>1.常见算法</h2><h3 id="有监督学习算法"><a href="#有监督学习算法" class="headerlink" title="有监督学习算法"></a>有监督学习算法</h3><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919155902254.png" alt="image-20240919155902254"></p>
<p>分类（Classification）</p>
<p>回归（Regression）</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919151938101.png" alt="image-20240919151938101"></p>
<h3 id="knn"><a href="#knn" class="headerlink" title="knn"></a>knn</h3><p>根据当前样本跟其他样本的距离来对当前样本进行分类，超参数k用于圈定范围</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919152115352.png" alt="image-20240919152115352"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919152500341.png" alt="image-20240919152500341"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919152851491.png" alt="image-20240919152851491"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919152916402.png" alt="image-20240919152916402"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">KNN（K-Nearest Neighbors，K最近邻）是一种简单而有效的监督学习算法，广泛用于分类和回归任务。KNN 的基本思想是通过计算样本之间的距离来进行预测，具体步骤如下：</span><br><span class="line"></span><br><span class="line">### 基本原理</span><br><span class="line"></span><br><span class="line">1. **训练阶段**：</span><br><span class="line">   - KNN 是一种懒惰学习（lazy learning）算法，它在训练阶段并不构建显式的模型，而是简单地存储训练数据。</span><br><span class="line"></span><br><span class="line">2. **预测阶段**：</span><br><span class="line">   - 对于一个新的输入样本，KNN 会计算该样本与训练集中所有样本的距离（通常使用欧几里得距离、曼哈顿距离等）。</span><br><span class="line">   - 找到距离最近的 K 个邻居。</span><br><span class="line">   - 对于分类任务，KNN 会根据这 K 个邻居的类别进行投票，选择出现次数最多的类别作为预测结果。</span><br><span class="line">   - 对于回归任务，KNN 会计算这 K 个邻居的平均值作为预测结果。</span><br><span class="line"></span><br><span class="line">### 选择 K 值</span><br><span class="line"></span><br><span class="line">- K 值的选择对 KNN 的性能有很大影响：</span><br><span class="line">  - **小的 K 值**（如 K=1）可能导致模型对噪声敏感，容易过拟合。</span><br><span class="line">  - **大的 K 值**可能导致模型过于平滑，无法捕捉到数据的局部结构，容易欠拟合。</span><br><span class="line">- 通常通过交叉验证来选择最佳的 K 值。</span><br><span class="line"></span><br><span class="line">### 距离度量</span><br><span class="line"></span><br><span class="line">KNN 依赖于距离度量来判断邻居的“近”与“远”。常用的距离度量包括：</span><br><span class="line"></span><br><span class="line">- **欧几里得距离**：最常用的距离度量，适用于连续特征。</span><br><span class="line">- **曼哈顿距离**：适用于某些特定场景，计算方式为各维度差值的绝对值之和。</span><br><span class="line">- **闵可夫斯基距离**：是欧几里得距离和曼哈顿距离的推广。</span><br><span class="line">- **余弦相似度**：适用于文本数据等高维稀疏数据。</span><br><span class="line"></span><br><span class="line">### 优缺点</span><br><span class="line"></span><br><span class="line">#### 优点：</span><br><span class="line">- 简单易懂，易于实现。</span><br><span class="line">- 不需要假设数据的分布。</span><br><span class="line">- 可以用于多分类问题。</span><br><span class="line"></span><br><span class="line">#### 缺点：</span><br><span class="line">- 计算复杂度高：在预测时需要计算所有训练样本的距离，随着数据量的增加，计算时间会显著增加。</span><br><span class="line">- 存储复杂度高：需要存储所有训练样本。</span><br><span class="line">- 对于高维数据，KNN 的性能可能下降（维度灾难）。</span><br><span class="line"></span><br><span class="line">### 应用场景</span><br><span class="line"></span><br><span class="line">KNN 被广泛应用于各种领域，包括但不限于：</span><br><span class="line"></span><br><span class="line">- 图像识别</span><br><span class="line">- 文本分类</span><br><span class="line">- 推荐系统</span><br><span class="line">- 医疗诊断</span><br><span class="line"></span><br><span class="line">### 示例代码</span><br><span class="line"></span><br><span class="line">以下是一个使用 Python 和 `scikit-learn` 库实现 KNN 分类的简单示例：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line"># 加载数据集</span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"># 划分训练集和测试集</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span><br><span class="line"></span><br><span class="line"># 创建 KNN 分类器</span><br><span class="line">k = 3</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 进行预测</span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"># 计算准确率</span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">print(f&#x27;Accuracy: &#123;accuracy:.2f&#125;&#x27;)</span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">这个示例中，我们使用了鸢尾花数据集（Iris dataset），将数据集划分为训练集和测试集，创建了一个 KNN 分类器，并计算了模型的准确率。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">在 KNN（K-Nearest Neighbors）算法中，分类的过程是基于已有的训练数据进行的。对于第一个样本（即在训练集中的第一个样本），它的类别是由训练数据本身决定的，而不是通过 KNN 算法来推断的。以下是详细的解释：</span><br><span class="line"></span><br><span class="line">### 1. 训练阶段</span><br><span class="line"></span><br><span class="line">在 KNN 的训练阶段，算法并不进行任何模型的构建或参数的学习。相反，它只是将训练数据存储在内存中。每个样本都有一个已知的类别标签，这些标签是在数据收集时就已经确定的。</span><br><span class="line"></span><br><span class="line">### 2. 第一个样本的类别</span><br><span class="line"></span><br><span class="line">- **已知类别**：在训练数据集中，每个样本都有一个对应的类别标签。第一个样本的类别是数据集中预先定义的，通常是在数据收集和标注阶段就已经确定的。</span><br><span class="line">- **存储**：KNN 仅仅是将这些样本及其对应的类别存储起来，以便在预测阶段使用。</span><br><span class="line"></span><br><span class="line">### 3. 预测阶段</span><br><span class="line"></span><br><span class="line">当 KNN 接收到一个新的样本进行分类时，它会执行以下步骤：</span><br><span class="line"></span><br><span class="line">1. **计算距离**：计算新样本与训练集中所有样本的距离。</span><br><span class="line">2. **选择 K 个邻居**：找到距离新样本最近的 K 个训练样本。</span><br><span class="line">3. **投票**：根据这 K 个邻居的类别进行投票，选择出现次数最多的类别作为新样本的预测类别。</span><br><span class="line"></span><br><span class="line">### 例子</span><br><span class="line"></span><br><span class="line">假设我们有一个简单的训练集：</span><br><span class="line"></span><br><span class="line">| 样本 | 特征1 | 特征2 | 类别 |</span><br><span class="line">|------|-------|-------|------|</span><br><span class="line">| 1    | 2.0   | 3.0   | A    |</span><br><span class="line">| 2    | 1.0   | 1.0   | B    |</span><br><span class="line">| 3    | 4.0   | 5.0   | A    |</span><br><span class="line">| 4    | 5.0   | 4.0   | B    |</span><br><span class="line"></span><br><span class="line">在这个例子中，样本 1 的类别是 A，样本 2 的类别是 B，样本 3 的类别是 A，样本 4 的类别是 B。这些类别是在训练数据准备阶段就已经确定的。</span><br><span class="line"></span><br><span class="line">当我们要对一个新的样本（例如，特征为 (3.0, 3.0)）进行分类时，KNN 会计算这个新样本与训练集中所有样本的距离，找到最近的 K 个邻居，然后根据这些邻居的类别进行投票，最终确定新样本的类别。</span><br><span class="line"></span><br><span class="line">### 总结</span><br><span class="line"></span><br><span class="line">KNN 算法的核心在于利用已有的训练数据进行分类，而不是通过算法推断第一个样本的类别。每个样本的类别是在数据准备阶段就已经确定的，KNN 仅仅是利用这些已知的类别进行预测。</span><br></pre></td></tr></table></figure>

<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919153806142.png" alt="image-20240919153806142"></p>
<h4 id="信息增益和特征选择"><a href="#信息增益和特征选择" class="headerlink" title="信息增益和特征选择"></a>信息增益和特征选择</h4><p>其实就是通过某个属性对熵的减弱（也就是信息增益的大小），如果信息增益比较强的话，就以这个属性作为特征。</p>
<p>可以理解为认识一个事物的过程就是提取特征然后下定义的过程，如此就可以以这个属性作为机器学习中的有效特征。（权重比较强的，大概是这样）</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919154219932.png" alt="image-20240919154219932"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919153124821.png" alt="image-20240919153124821"></p>
<p>根据信息增益，将每个属性的权重计算出来，其实就是得到最佳特征的过程。</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919154519362.png" alt="image-20240919154519362"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919154829058.png" alt="image-20240919154829058"></p>
<p>选择完第一个特征之后根据特征进行分支，然后再选出第二个特征，以此类推。</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919155004523.png" alt="image-20240919155004523"></p>
<p>根据循环的停止条件来判断是否继续分支（如纯度，循环次数等）</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919155149094.png" alt="image-20240919155149094"></p>
<p>全部停止之后生成决策树</p>
<h4 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">基尼系数（Gini Coefficient）是一个用于衡量收入或财富分配不平等程度的统计指标。它由意大利统计学家科拉多·基尼（Corrado Gini）于 1912 年提出。基尼系数的值范围从 0 到 1，或者可以表示为 0% 到 100%。以下是基尼系数的详细介绍：</span><br><span class="line"></span><br><span class="line">### 1. 基尼系数的定义</span><br><span class="line"></span><br><span class="line">- **完全平等**：如果一个国家或地区的收入分配完全平等，所有人都有相同的收入，则基尼系数为 0。</span><br><span class="line">- **完全不平等**：如果一个国家或地区的收入分配完全不平等，所有收入都集中在一个人手中，其余人没有收入，则基尼系数为 1。</span><br><span class="line"></span><br><span class="line">### 2. 计算方法</span><br><span class="line"></span><br><span class="line">基尼系数可以通过洛伦兹曲线（Lorenz Curve）来计算。洛伦兹曲线是一个图形，表示收入分配的累积比例。基尼系数可以通过以下公式计算：</span><br><span class="line"></span><br><span class="line">\[</span><br><span class="line">G = \frac&#123;A&#125;&#123;A + B&#125;</span><br><span class="line">\]</span><br><span class="line"></span><br><span class="line">其中：</span><br><span class="line">- \(A\) 是洛伦兹曲线与完全平等线之间的面积。</span><br><span class="line">- \(B\) 是洛伦兹曲线下方的面积。</span><br><span class="line"></span><br><span class="line">基尼系数也可以通过以下公式计算：</span><br><span class="line"></span><br><span class="line">\[</span><br><span class="line">G = 1 - 2 \int_0^1 L(p) \, dp</span><br><span class="line">\]</span><br><span class="line"></span><br><span class="line">其中 \(L(p)\) 是洛伦兹曲线的函数，\(p\) 是收入的累积比例。</span><br><span class="line"></span><br><span class="line">### 3. 基尼系数的解释</span><br><span class="line"></span><br><span class="line">- **基尼系数为 0**：表示完全平等，所有人收入相同。</span><br><span class="line">- **基尼系数为 0.2**（或 20%）：表示收入分配相对平等。</span><br><span class="line">- **基尼系数为 0.5**（或 50%）：表示收入分配不平等。</span><br><span class="line">- **基尼系数为 1**（或 100%）：表示完全不平等，所有收入集中在一个人手中。</span><br><span class="line"></span><br><span class="line">### 4. 应用</span><br><span class="line"></span><br><span class="line">基尼系数广泛应用于经济学、社会学和公共政策等领域，主要用于：</span><br><span class="line"></span><br><span class="line">- 衡量国家或地区的收入不平等程度。</span><br><span class="line">- 评估政策对收入分配的影响。</span><br><span class="line">- 比较不同国家或地区的收入分配状况。</span><br><span class="line"></span><br><span class="line">### 5. 优缺点</span><br><span class="line"></span><br><span class="line">#### 优点：</span><br><span class="line">- **简单易懂**：基尼系数的计算和解释相对简单。</span><br><span class="line">- **广泛适用**：可以用于不同国家、地区和时间段的比较。</span><br><span class="line"></span><br><span class="line">#### 缺点：</span><br><span class="line">- **信息损失**：基尼系数只提供了收入分配的一个总体指标，可能忽略了分配的具体情况。</span><br><span class="line">- **不敏感于极端值**：基尼系数对极端收入的变化不够敏感，可能无法反映收入分配的真实情况。</span><br><span class="line"></span><br><span class="line">### 总结</span><br><span class="line"></span><br><span class="line">基尼系数是一个重要的经济指标，用于衡量收入或财富分配的不平等程度。通过基尼系数，政策制定者和研究人员可以更好地理解和分析社会经济问题，制定相应的政策来改善收入分配状况。</span><br></pre></td></tr></table></figure>



<h4 id="决策树停止条件"><a href="#决策树停止条件" class="headerlink" title="决策树停止条件"></a>决策树停止条件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">在构建决策树时，停止分支的条件是非常重要的，因为它直接影响到模型的复杂性和泛化能力。以下是一些常见的停止分支条件：</span><br><span class="line"></span><br><span class="line">### 1. **达到最大深度**</span><br><span class="line"></span><br><span class="line">- **最大深度限制**：设置一个最大深度参数，限制决策树的深度。当树的深度达到这个限制时，停止进一步分支。这有助于防止过拟合。</span><br><span class="line"></span><br><span class="line">### 2. **最小样本数**</span><br><span class="line"></span><br><span class="line">- **最小样本数限制**：在某个节点上，如果样本数少于设定的最小样本数（如 `min_samples_split`），则停止分支。这可以防止在样本数量非常少的情况下进行分支，从而减少过拟合的风险。</span><br><span class="line"></span><br><span class="line">### 3. **信息增益或基尼指数的阈值**</span><br><span class="line"></span><br><span class="line">- **信息增益阈值**：如果在某个节点上进行分支所获得的信息增益小于设定的阈值，则停止分支。信息增益越小，表示该特征对分类的贡献越小。</span><br><span class="line">- **基尼指数阈值**：类似地，如果基尼指数的减少小于某个阈值，也可以停止分支。</span><br><span class="line"></span><br><span class="line">### 4. **纯度阈值**</span><br><span class="line"></span><br><span class="line">- **节点纯度**：如果某个节点的样本已经完全属于同一类别（即节点的纯度达到 100%），则停止分支。此时，节点可以被标记为叶子节点。</span><br><span class="line"></span><br><span class="line">### 5. **达到预设的叶子节点数量**</span><br><span class="line"></span><br><span class="line">- **叶子节点数量限制**：可以设置一个最大叶子节点数量，当达到这个数量时，停止分支。这有助于控制模型的复杂性。</span><br><span class="line"></span><br><span class="line">### 6. **交叉验证**</span><br><span class="line"></span><br><span class="line">- **交叉验证**：在训练过程中，可以使用交叉验证来评估模型的性能。如果进一步分支导致验证集的性能下降，则可以停止分支。</span><br><span class="line"></span><br><span class="line">### 7. **其他启发式方法**</span><br><span class="line"></span><br><span class="line">- **启发式方法**：有时可以使用其他启发式方法来决定是否停止分支，例如基于领域知识或经验法则。</span><br><span class="line"></span><br><span class="line">### 总结</span><br><span class="line"></span><br><span class="line">决策树的停止分支条件可以通过多种方式来设定，通常是结合多种条件来确保模型的有效性和泛化能力。合理的停止条件可以帮助避免过拟合，提高模型在未见数据上的表现。</span><br></pre></td></tr></table></figure>

<h4 id="生成决策树算法"><a href="#生成决策树算法" class="headerlink" title="生成决策树算法"></a>生成决策树算法</h4><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919155228797.png" alt="image-20240919155228797"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919155652194.png" alt="image-20240919155652194"></p>
<h5 id="id3系列算法"><a href="#id3系列算法" class="headerlink" title="id3系列算法"></a>id3系列算法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">ID3（Iterative Dichotomiser 3）是一种用于构建决策树的算法，由 Ross Quinlan 在 1986 年提出。ID3 算法是基于信息增益的，主要用于分类任务。ID3 算法的基本思想是通过选择最能区分样本的特征来构建决策树。ID3 算法的后续版本和相关算法包括 C4.5 和 C5.0。以下是这些算法的简要介绍：</span><br><span class="line"></span><br><span class="line">### 1. ID3 算法</span><br><span class="line"></span><br><span class="line">- **基本原理**：ID3 使用信息增益作为特征选择的标准。信息增益衡量的是通过选择某个特征来减少的不确定性。</span><br><span class="line">- **构建过程**：</span><br><span class="line">  1. 计算每个特征的信息增益。</span><br><span class="line">  2. 选择信息增益最大的特征作为当前节点的分裂特征。</span><br><span class="line">  3. 根据该特征的不同取值划分数据集。</span><br><span class="line">  4. 对每个子集递归地重复上述过程，直到满足停止条件（如达到最大深度、样本数不足等）。</span><br><span class="line">- **优缺点**：</span><br><span class="line">  - 优点：简单易懂，易于实现。</span><br><span class="line">  - 缺点：容易过拟合，特别是在特征较多时；对噪声敏感。</span><br><span class="line"></span><br><span class="line">### 2. C4.5 算法</span><br><span class="line"></span><br><span class="line">C4.5 是 ID3 的改进版本，由 Ross Quinlan 在 1993 年提出。它在 ID3 的基础上进行了多项改进：</span><br><span class="line"></span><br><span class="line">- **使用增益比**：C4.5 使用增益比（Gain Ratio）而不是信息增益来选择特征。增益比考虑了特征的取值数量，避免了 ID3 偏向于选择取值较多的特征的问题。</span><br><span class="line">  </span><br><span class="line">  \[</span><br><span class="line">  \text&#123;Gain Ratio&#125; = \frac&#123;\text&#123;Information Gain&#125;&#125;&#123;\text&#123;Split Information&#125;&#125;</span><br><span class="line">  \]</span><br><span class="line"></span><br><span class="line">- **处理连续特征**：C4.5 可以处理连续特征，通过选择合适的阈值将其离散化。</span><br><span class="line">- **处理缺失值**：C4.5 可以处理缺失值，通过将缺失值视为一个额外的类别或通过加权来处理。</span><br><span class="line">- **剪枝**：C4.5 引入了剪枝技术，以减少过拟合。剪枝是在树构建完成后，通过移除一些不必要的节点来简化树结构。</span><br><span class="line"></span><br><span class="line">### 3. C5.0 算法</span><br><span class="line"></span><br><span class="line">C5.0 是 C4.5 的进一步改进版本，具有以下特点：</span><br><span class="line"></span><br><span class="line">- **更高的效率**：C5.0 在构建决策树时比 C4.5 更快，尤其是在处理大数据集时。</span><br><span class="line">- **更好的准确性**：C5.0 在分类准确性上通常优于 C4.5。</span><br><span class="line">- **Boosting**：C5.0 支持 Boosting 技术，可以通过组合多个弱分类器来提高模型的性能。</span><br><span class="line">- **更好的内存管理**：C5.0 在内存使用上进行了优化，适合处理大规模数据集。</span><br><span class="line"></span><br><span class="line">### 总结</span><br><span class="line"></span><br><span class="line">ID3、C4.5 和 C5.0 是决策树算法的经典代表。ID3 是最早的版本，主要使用信息增益进行特征选择；C4.5 改进了 ID3，使用增益比并引入了处理连续特征和缺失值的能力；C5.0 则在效率和准确性上进一步提升，并引入了 Boosting 技术。决策树算法因其直观性和易于解释的特性，在许多领域得到了广泛应用。</span><br></pre></td></tr></table></figure>

<h5 id="cart算法"><a href="#cart算法" class="headerlink" title="cart算法"></a>cart算法</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">CART（Classification and Regression Trees）是一种用于分类和回归的决策树算法，由 Breiman 等人在 1986 年提出。CART 是一种非常灵活的算法，能够处理各种类型的数据，并且在许多机器学习任务中得到了广泛应用。以下是 CART 算法的主要特点和工作原理：</span><br><span class="line"></span><br><span class="line">### 1. 基本原理</span><br><span class="line"></span><br><span class="line">CART 可以用于两种主要任务：</span><br><span class="line">- **分类**：用于分类任务，输出类别标签。</span><br><span class="line">- **回归**：用于回归任务，输出连续值。</span><br><span class="line"></span><br><span class="line">### 2. 特征选择</span><br><span class="line"></span><br><span class="line">CART 在构建决策树时，使用不同的标准来选择特征：</span><br><span class="line"></span><br><span class="line">- **分类任务**：CART 使用基尼指数（Gini Index）作为特征选择的标准。基尼指数衡量的是一个节点的不纯度，计算公式为：</span><br><span class="line"></span><br><span class="line">  \[</span><br><span class="line">  Gini(D) = 1 - \sum_&#123;i=1&#125;^&#123;C&#125; p_i^2</span><br><span class="line">  \]</span><br><span class="line"></span><br><span class="line">  其中 \(p_i\) 是类别 \(i\) 在数据集 \(D\) 中的比例，\(C\) 是类别的总数。CART 会选择使基尼指数最小化的特征进行分裂。</span><br><span class="line"></span><br><span class="line">- **回归任务**：CART 使用均方误差（Mean Squared Error, MSE）作为特征选择的标准。均方误差计算公式为：</span><br><span class="line"></span><br><span class="line">  \[</span><br><span class="line">  MSE = \frac&#123;1&#125;&#123;N&#125; \sum_&#123;i=1&#125;^&#123;N&#125; (y_i - \bar&#123;y&#125;)^2</span><br><span class="line">  \]</span><br><span class="line"></span><br><span class="line">  其中 \(y_i\) 是实际值，\(\bar&#123;y&#125;\) 是预测值。CART 会选择使均方误差最小化的特征进行分裂。</span><br><span class="line"></span><br><span class="line">### 3. 树的构建</span><br><span class="line"></span><br><span class="line">CART 的树构建过程如下：</span><br><span class="line"></span><br><span class="line">1. **选择特征**：计算所有特征的基尼指数（分类）或均方误差（回归），选择最优特征进行分裂。</span><br><span class="line">2. **分裂节点**：根据选择的特征和最佳阈值将数据集分成两个子集。</span><br><span class="line">3. **递归构建**：对每个子集递归地重复上述过程，直到满足停止条件（如达到最大深度、样本数不足、节点纯度达到 100% 等）。</span><br><span class="line">4. **生成叶子节点**：当停止条件满足时，生成叶子节点，并为其分配类别标签（分类）或预测值（回归）。</span><br><span class="line"></span><br><span class="line">### 4. 剪枝</span><br><span class="line"></span><br><span class="line">CART 算法通常会在树构建完成后进行剪枝，以减少过拟合。剪枝的过程包括：</span><br><span class="line"></span><br><span class="line">- **预剪枝**：在树构建过程中，设置一些条件（如最小样本数、最大深度等），在满足条件时停止分裂。</span><br><span class="line">- **后剪枝**：在树构建完成后，评估每个节点的贡献，移除一些不必要的节点，以简化树结构。</span><br><span class="line"></span><br><span class="line">### 5. 优缺点</span><br><span class="line"></span><br><span class="line">#### 优点：</span><br><span class="line">- **灵活性**：CART 可以处理分类和回归问题。</span><br><span class="line">- **易于解释**：决策树的结构直观，易于理解和解释。</span><br><span class="line">- **处理缺失值**：CART 可以处理缺失值，具有一定的鲁棒性。</span><br><span class="line"></span><br><span class="line">#### 缺点：</span><br><span class="line">- **过拟合**：CART 容易在训练数据上过拟合，尤其是在数据量较小或特征较多时。</span><br><span class="line">- **不稳定性**：小的变化可能导致树结构的显著变化。</span><br><span class="line"></span><br><span class="line">### 总结</span><br><span class="line"></span><br><span class="line">CART 是一种强大的决策树算法，能够处理分类和回归任务。通过使用基尼指数和均方误差作为特征选择标准，CART 能够有效地构建决策树。剪枝技术的引入有助于提高模型的泛化能力。CART 算法在许多实际应用中得到了广泛的应用，包括金融、医疗、市场营销等领域。</span><br></pre></td></tr></table></figure>

<h3 id="无监督学习算法"><a href="#无监督学习算法" class="headerlink" title="无监督学习算法"></a>无监督学习算法</h3><p>没有给出结论的样本数据</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919160045167.png" alt="image-20240919160045167"></p>
<p>聚类算法</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919160226243.png" alt="image-20240919160226243"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919160657713.png" alt="image-20240919160657713"></p>
<h3 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h3><p>超参数k为中心的个数（k的个数对结果的影响很大）</p>
<p>不断迭代计算出类的中心，直到类的中心不再变化</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919161059262.png" alt="image-20240919161059262"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919161541307.png" alt="image-20240919161541307"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919161654743.png" alt="image-20240919161654743"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">K-means 算法是一种广泛使用的聚类算法，旨在将数据集分成 K 个簇（clusters），使得同一簇内的数据点尽可能相似，而不同簇之间的数据点尽可能不同。K-means 算法简单易懂，计算效率高，适用于大规模数据集。以下是 K-means 算法的详细介绍：</span><br><span class="line"></span><br><span class="line">### 1. 算法原理</span><br><span class="line"></span><br><span class="line">K-means 算法的基本步骤如下：</span><br><span class="line"></span><br><span class="line">1. **选择 K 值**：确定要将数据集分成多少个簇（K 的值）。</span><br><span class="line">2. **初始化中心**：随机选择 K 个数据点作为初始簇中心（centroids）。</span><br><span class="line">3. **分配簇**：</span><br><span class="line">   - 对于数据集中的每个数据点，计算其与 K 个簇中心的距离（通常使用欧几里得距离）。</span><br><span class="line">   - 将每个数据点分配给距离最近的簇中心。</span><br><span class="line">4. **更新中心**：</span><br><span class="line">   - 重新计算每个簇的中心，方法是计算分配给该簇的所有数据点的均值。</span><br><span class="line">5. **重复步骤 3 和 4**：直到簇中心不再发生变化（收敛）或达到预设的迭代次数。</span><br><span class="line"></span><br><span class="line">### 2. 算法的数学表示</span><br><span class="line"></span><br><span class="line">- **距离计算**：对于数据点 \(x_i\) 和簇中心 \(c_k\)，使用欧几里得距离计算：</span><br><span class="line"></span><br><span class="line">\[</span><br><span class="line">d(x_i, c_k) = \sqrt&#123;\sum_&#123;j=1&#125;^&#123;n&#125; (x_&#123;ij&#125; - c_&#123;kj&#125;)^2&#125;</span><br><span class="line">\]</span><br><span class="line"></span><br><span class="line">- **簇中心更新**：对于簇 \(C_k\) 中的所有数据点 \(x_i\)，簇中心 \(c_k\) 更新为：</span><br><span class="line"></span><br><span class="line">\[</span><br><span class="line">c_k = \frac&#123;1&#125;&#123;|C_k|&#125; \sum_&#123;x_i \in C_k&#125; x_i</span><br><span class="line">\]</span><br><span class="line"></span><br><span class="line">### 3. K-means 的优缺点</span><br><span class="line"></span><br><span class="line">#### 优点：</span><br><span class="line">- **简单易懂**：K-means 算法易于实现和理解。</span><br><span class="line">- **高效**：对于大规模数据集，K-means 算法的计算效率较高，时间复杂度为 \(O(n \cdot k \cdot i)\)，其中 \(n\) 是数据点数量，\(k\) 是簇的数量，\(i\) 是迭代次数。</span><br><span class="line">- **可扩展性**：适用于大规模数据集。</span><br><span class="line"></span><br><span class="line">#### 缺点：</span><br><span class="line">- **K 值的选择**：需要事先指定 K 值，选择不当可能导致聚类效果不佳。</span><br><span class="line">- **对初始值敏感**：不同的初始簇中心可能导致不同的聚类结果，可能会陷入局部最优解。</span><br><span class="line">- **对噪声和离群点敏感**：K-means 对噪声和离群点敏感，可能影响聚类结果。</span><br><span class="line">- **假设簇是球形的**：K-means 假设簇是球形的，且各簇的大小相似，这在某些情况下可能不成立。</span><br><span class="line"></span><br><span class="line">### 4. K-means 的改进</span><br><span class="line"></span><br><span class="line">为了克服 K-means 的一些缺点，研究人员提出了一些改进算法，例如：</span><br><span class="line"></span><br><span class="line">- **K-means++**：一种改进的初始化方法，通过选择更远离已有簇中心的点作为初始中心，减少对初始值的敏感性。</span><br><span class="line">- **层次聚类**：结合 K-means 和层次聚类的方法，先进行层次聚类，再使用 K-means 进行细化。</span><br><span class="line">- **DBSCAN**：一种基于密度的聚类算法，能够识别任意形状的簇，并且对噪声和离群点具有更好的鲁棒性。</span><br><span class="line"></span><br><span class="line">### 5. 应用场景</span><br><span class="line"></span><br><span class="line">K-means 算法广泛应用于多个领域，包括但不限于：</span><br><span class="line"></span><br><span class="line">- 图像压缩</span><br><span class="line">- 市场细分</span><br><span class="line">- 社交网络分析</span><br><span class="line">- 文本聚类</span><br><span class="line">- 生物信息学</span><br><span class="line"></span><br><span class="line">### 示例代码</span><br><span class="line"></span><br><span class="line">以下是一个使用 Python 和 `scikit-learn` 库实现 K-means 聚类的简单示例：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line"></span><br><span class="line"># 生成示例数据</span><br><span class="line">X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)</span><br><span class="line"></span><br><span class="line"># 创建 K-means 模型</span><br><span class="line">kmeans = KMeans(n_clusters=4)</span><br><span class="line">kmeans.fit(X)</span><br><span class="line"></span><br><span class="line"># 获取聚类结果</span><br><span class="line">y_kmeans = kmeans.predict(X)</span><br><span class="line"></span><br><span class="line"># 可视化结果</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap=&#x27;viridis&#x27;)</span><br><span class="line">centers = kmeans.cluster_centers_</span><br><span class="line">plt.scatter(centers[:, 0], centers[:, 1], c=&#x27;red&#x27;, s=200, alpha=0.75, marker=&#x27;X&#x27;)</span><br><span class="line">plt.title(&#x27;K-means Clustering&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">在这个示例中，我们生成了一个包含 300 个样本的示例数据集，并使用 K-means 算法将其聚类为 4 个簇。最后，我们可视化了聚类结果和簇中心。</span><br></pre></td></tr></table></figure>

<h4 id="关联规则"><a href="#关联规则" class="headerlink" title="关联规则"></a>关联规则</h4><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919161748278.png" alt="image-20240919161748278"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919172441742.png" alt="image-20240919172441742"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919211435421.png" alt="image-20240919211435421"></p>
<p>支持度就是这个集合在样本中占的比例，置信度就是所占比例是否具有收录到项集中的意义（超过置信度才收录）</p>
<p>注意下一集合是由上层集合推导出来的，而不是遍历所有集合</p>
<h3 id="其他算法"><a href="#其他算法" class="headerlink" title="其他算法"></a>其他算法</h3><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919211551241.png" alt="image-20240919211551241"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919211702109.png" alt="image-20240919211702109"></p>
<h4 id="投票模型"><a href="#投票模型" class="headerlink" title="投票模型"></a>投票模型</h4><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919212402591.png" alt="image-20240919212402591"></p>
<h4 id="错误权重叠加迭代模型"><a href="#错误权重叠加迭代模型" class="headerlink" title="错误权重叠加迭代模型"></a>错误权重叠加迭代模型</h4><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919212500592.png" alt="image-20240919212500592"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919212642585.png" alt="image-20240919212642585"></p>
<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919212747222.png" alt="image-20240919212747222"></p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919212835401.png" alt="image-20240919212835401"></p>
<h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919212939330.png" alt="image-20240919212939330"></p>
<h4 id="增强学习"><a href="#增强学习" class="headerlink" title="增强学习"></a>增强学习</h4><p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919213038648.png" alt="image-20240919213038648"></p>
<h4 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h4><p>（增强了泛用性？</p>
<p><img src="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/image-20240919213050497.png" alt="image-20240919213050497"></p>
<h2 id="课程链接"><a href="#课程链接" class="headerlink" title="课程链接"></a>课程链接</h2><p>所有课程内容均来自阿里云：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://edu.aliyun.com/course/313926</span><br><span class="line">https://developer.aliyun.com/learning/roadmap/ai</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://zx2023qj.github.io">zx</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zx2023qj.github.io/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/">https://zx2023qj.github.io/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zx2023qj.github.io" target="_blank">zx's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/ai/">ai</a></div><div class="post_share"><div class="social-share" data-image="/img/3.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/09/18/2024%E4%B8%AD%E7%A7%8B/" title="2024中秋"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">2024中秋</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/3.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">zx</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zx2023qj"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8"><span class="toc-number">1.</span> <span class="toc-text">机器学习入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text">1.常见算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.1.</span> <span class="toc-text">有监督学习算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#knn"><span class="toc-number">1.1.2.</span> <span class="toc-text">knn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.1.3.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">信息熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E5%92%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">信息增益和特征选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0"><span class="toc-number">1.1.3.3.</span> <span class="toc-text">基尼系数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%81%9C%E6%AD%A2%E6%9D%A1%E4%BB%B6"><span class="toc-number">1.1.3.4.</span> <span class="toc-text">决策树停止条件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.3.5.</span> <span class="toc-text">生成决策树算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#id3%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.3.5.1.</span> <span class="toc-text">id3系列算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#cart%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.3.5.2.</span> <span class="toc-text">cart算法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.4.</span> <span class="toc-text">无监督学习算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.5.</span> <span class="toc-text">K-Means算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99"><span class="toc-number">1.1.5.1.</span> <span class="toc-text">关联规则</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.6.</span> <span class="toc-text">其他算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%95%E7%A5%A8%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.6.1.</span> <span class="toc-text">投票模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%94%99%E8%AF%AF%E6%9D%83%E9%87%8D%E5%8F%A0%E5%8A%A0%E8%BF%AD%E4%BB%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.6.2.</span> <span class="toc-text">错误权重叠加迭代模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">1.1.6.3.</span> <span class="toc-text">随机森林</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.6.4.</span> <span class="toc-text">深度学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.6.5.</span> <span class="toc-text">增强学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.6.6.</span> <span class="toc-text">迁移学习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E7%A8%8B%E9%93%BE%E6%8E%A5"><span class="toc-number">1.2.</span> <span class="toc-text">课程链接</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/" title="机器学习1">机器学习1</a><time datetime="2024-09-19T13:38:09.000Z" title="发表于 2024-09-19 21:38:09">2024-09-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/18/2024%E4%B8%AD%E7%A7%8B/" title="2024中秋">2024中秋</a><time datetime="2024-09-18T04:20:36.000Z" title="发表于 2024-09-18 12:20:36">2024-09-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/07/pwn101-110%E6%95%B4%E6%95%B0%E5%AE%89%E5%85%A8/" title="pwn101-110整数安全">pwn101-110整数安全</a><time datetime="2024-09-07T07:57:52.000Z" title="发表于 2024-09-07 15:57:52">2024-09-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/05/%E5%86%8D%E6%B0%B4%E4%B8%80%E7%AF%87%E6%97%A5%E5%B8%B8/" title="再水一篇日常">再水一篇日常</a><time datetime="2024-09-05T15:53:08.000Z" title="发表于 2024-09-05 23:53:08">2024-09-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/05/pwn91-100%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%AD%97%E7%AC%A6%E4%B8%B2/" title="pwn91-100格式化字符串">pwn91-100格式化字符串</a><time datetime="2024-09-05T15:47:36.000Z" title="发表于 2024-09-05 23:47:36">2024-09-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By zx</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>